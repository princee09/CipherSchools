{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Purchase Likelihood Prediction\n",
    "\n",
    "**Goal:** Predict whether a customer will buy a product using machine learning\n",
    "\n",
    "**Features:**\n",
    "- Time spent on website\n",
    "- Age\n",
    "- Gender\n",
    "- Ads clicked\n",
    "- Previous purchases\n",
    "\n",
    "**Type:** Binary Classification\n",
    "**Models:** Logistic Regression & Decision Tree\n",
    "**Application:** E-commerce behavior modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('product_purchase_ - product_purchase_impure.csv.csv')\n",
    "\n",
    "print('Dataset loaded successfully!')\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print('\\nFirst 5 rows:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print('Dataset Info:')\n",
    "print(df.info())\n",
    "print('\\nDataset Description:')\n",
    "print(df.describe())\n",
    "print('\\nColumn Names:')\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('Missing Values:')\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(f'\\nTotal missing values: {missing_values.sum()}')\n",
    "print(f'Percentage of missing values: {(missing_values.sum() / len(df)) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in each column\n",
    "print('Unique values in each column:')\n",
    "for col in df.columns:\n",
    "    print(f'{col}: {df[col].nunique()} unique values')\n",
    "    if df[col].nunique() < 10:\n",
    "        print(f'  Values: {df[col].unique()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "print('Original dataset shape:', df.shape)\n",
    "print('Starting data cleaning process...')\n",
    "\n",
    "# Check data types and identify issues\n",
    "print('Data types:')\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data type issues - convert numeric columns\n",
    "numeric_columns = ['TimeOnSite', 'Age', 'AdsClicked', 'PreviousPurchases', 'Purchase']\n",
    "\n",
    "for col in numeric_columns:\n",
    "    # Convert to numeric, errors='coerce' will turn invalid values to NaN\n",
    "    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "print('Data types after conversion:')\n",
    "print(df_clean.dtypes)\n",
    "\n",
    "print('Missing values after conversion:')\n",
    "print(df_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers in Age column (values like 112.4178184 seem unrealistic)\n",
    "print('Age statistics before cleaning:')\n",
    "print(df_clean['Age'].describe())\n",
    "\n",
    "# Remove unrealistic age values (assuming reasonable age range 18-80)\n",
    "df_clean.loc[df_clean['Age'] > 80, 'Age'] = np.nan\n",
    "df_clean.loc[df_clean['Age'] < 18, 'Age'] = np.nan\n",
    "\n",
    "print('Age statistics after cleaning:')\n",
    "print(df_clean['Age'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers in other numeric columns\n",
    "# Check for unrealistic values in AdsClicked and TimeOnSite\n",
    "print('Checking for outliers in numeric columns:')\n",
    "\n",
    "for col in ['TimeOnSite', 'AdsClicked', 'PreviousPurchases']:\n",
    "    print(f'\\n{col} statistics:')\n",
    "    print(df_clean[col].describe())\n",
    "    \n",
    "    # Remove extreme outliers using IQR method\n",
    "    Q1 = df_clean[col].quantile(0.25)\n",
    "    Q3 = df_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    outliers = (df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)\n",
    "    print(f'Outliers found: {outliers.sum()}')\n",
    "    \n",
    "    # Replace outliers with NaN\n",
    "    df_clean.loc[outliers, col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print('Missing values before imputation:')\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# Impute missing values\n",
    "# For numeric columns, use median\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "numeric_cols = ['TimeOnSite', 'Age', 'AdsClicked', 'PreviousPurchases']\n",
    "\n",
    "df_clean[numeric_cols] = numeric_imputer.fit_transform(df_clean[numeric_cols])\n",
    "\n",
    "# For Gender, use mode\n",
    "gender_mode = df_clean['Gender'].mode()[0]\n",
    "df_clean['Gender'].fillna(gender_mode, inplace=True)\n",
    "\n",
    "# For Purchase (target), remove rows with missing values\n",
    "df_clean = df_clean.dropna(subset=['Purchase'])\n",
    "\n",
    "print('Missing values after imputation:')\n",
    "print(df_clean.isnull().sum())\n",
    "print(f'\\nFinal dataset shape: {df_clean.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables
    print('Encoding categorical variables...')
    
    # Encode Gender
    le_gender = LabelEncoder()
    df_clean['Gender_encoded'] = le_gender.fit_transform(df_clean['Gender'])
    
    print('Gender encoding:')
    gender_mapping = dict(zip(le_gender.classes_, le_gender.transform(le_gender.classes_)))
    print(gender_mapping)
    
    # Fix Purchase variable for binary classification
    print('\nOriginal Purchase values:')
    print(df_clean['Purchase'].unique())
    print(f'Purchase value counts:')
    print(df_clean['Purchase'].value_counts())
    
    # Convert Purchase to proper binary (0 or 1)
    # Treat any value > 0 as 1 (purchase), and 0 as 0 (no purchase)
    df_clean['Purchase'] = (df_clean['Purchase'] > 0).astype(int)
    
    print('\nCorrected Purchase values:')
    print(df_clean['Purchase'].unique())
    print('\nTarget variable distribution:')
    print(df_clean['Purchase'].value_counts())
    print(f'Purchase rate: {df_clean["Purchase"].mean():.2%}')
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Exploratory Data Analysis - Product Purchase Dataset', fontsize=16)\n",
    "\n",
    "# 1. Target variable distribution\n",
    "df_clean['Purchase'].value_counts().plot(kind='bar', ax=axes[0,0])\n",
    "axes[0,0].set_title('Purchase Distribution')\n",
    "axes[0,0].set_xlabel('Purchase (0=No, 1=Yes)')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "\n",
    "# 2. Age distribution\n",
    "df_clean['Age'].hist(bins=20, ax=axes[0,1])\n",
    "axes[0,1].set_title('Age Distribution')\n",
    "axes[0,1].set_xlabel('Age')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Time on Site distribution\n",
    "df_clean['TimeOnSite'].hist(bins=20, ax=axes[0,2])\n",
    "axes[0,2].set_title('Time on Site Distribution')\n",
    "axes[0,2].set_xlabel('Time on Site')\n",
    "axes[0,2].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Gender vs Purchase\n",
    "pd.crosstab(df_clean['Gender'], df_clean['Purchase']).plot(kind='bar', ax=axes[1,0])\n",
    "axes[1,0].set_title('Gender vs Purchase')\n",
    "axes[1,0].set_xlabel('Gender')\n",
    "axes[1,0].set_ylabel('Count')\n",
    "axes[1,0].legend(['No Purchase', 'Purchase'])\n",
    "\n",
    "# 5. Ads Clicked distribution\n",
    "df_clean['AdsClicked'].hist(bins=15, ax=axes[1,1])\n",
    "axes[1,1].set_title('Ads Clicked Distribution')\n",
    "axes[1,1].set_xlabel('Ads Clicked')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "\n",
    "# 6. Previous Purchases distribution\n",
    "df_clean['PreviousPurchases'].hist(bins=15, ax=axes[1,2])\n",
    "axes[1,2].set_title('Previous Purchases Distribution')\n",
    "axes[1,2].set_xlabel('Previous Purchases')\n",
    "axes[1,2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Select numeric columns for correlation\n",
    "corr_cols = ['TimeOnSite', 'Age', 'Gender_encoded', 'AdsClicked', 'PreviousPurchases', 'Purchase']\n",
    "correlation_matrix = df_clean[corr_cols].corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Correlation with Purchase:')\n",
    "purchase_corr = correlation_matrix['Purchase'].sort_values(ascending=False)\n",
    "print(purchase_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature analysis by purchase behavior\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Feature Analysis by Purchase Behavior', fontsize=16)\n",
    "\n",
    "# Time on Site by Purchase\n",
    "df_clean.boxplot(column='TimeOnSite', by='Purchase', ax=axes[0,0])\n",
    "axes[0,0].set_title('Time on Site by Purchase')\n",
    "axes[0,0].set_xlabel('Purchase (0=No, 1=Yes)')\n",
    "\n",
    "# Age by Purchase\n",
    "df_clean.boxplot(column='Age', by='Purchase', ax=axes[0,1])\n",
    "axes[0,1].set_title('Age by Purchase')\n",
    "axes[0,1].set_xlabel('Purchase (0=No, 1=Yes)')\n",
    "\n",
    "# Ads Clicked by Purchase\n",
    "df_clean.boxplot(column='AdsClicked', by='Purchase', ax=axes[1,0])\n",
    "axes[1,0].set_title('Ads Clicked by Purchase')\n",
    "axes[1,0].set_xlabel('Purchase (0=No, 1=Yes)')\n",
    "\n",
    "# Previous Purchases by Purchase\n",
    "df_clean.boxplot(column='PreviousPurchases', by='Purchase', ax=axes[1,1])\n",
    "axes[1,1].set_title('Previous Purchases by Purchase')\n",
    "axes[1,1].set_xlabel('Purchase (0=No, 1=Yes)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_columns = ['TimeOnSite', 'Age', 'Gender_encoded', 'AdsClicked', 'PreviousPurchases']\n",
    "X = df_clean[feature_columns]\n",
    "y = df_clean['Purchase']\n",
    "\n",
    "print('Feature matrix shape:', X.shape)\n",
    "print('Target vector shape:', y.shape)\n",
    "print('\\nFeatures:')\n",
    "print(X.columns.tolist())\n",
    "\n",
    "print('\\nFeature statistics:')\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print('Training set shape:', X_train.shape)\n",
    "print('Test set shape:', X_test.shape)\n",
    "print('\\nTraining set target distribution:')\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print('\\nTest set target distribution:')\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('Features scaled successfully!')\n",
    "print('Scaled training set shape:', X_train_scaled.shape)\n",
    "print('Scaled test set shape:', X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_train_pred = lr_model.predict(X_train_scaled)\n",
    "lr_test_pred = lr_model.predict(X_test_scaled)\n",
    "lr_test_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "lr_train_accuracy = accuracy_score(y_train, lr_train_pred)\n",
    "lr_test_accuracy = accuracy_score(y_test, lr_test_pred)\n",
    "lr_auc = roc_auc_score(y_test, lr_test_proba)\n",
    "\n",
    "print('=== LOGISTIC REGRESSION RESULTS ===')\n",
    "print(f'Training Accuracy: {lr_train_accuracy:.4f}')\n",
    "print(f'Test Accuracy: {lr_test_accuracy:.4f}')\n",
    "print(f'AUC Score: {lr_auc:.4f}')\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, lr_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Logistic Regression\n",
    "lr_feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'coefficient': lr_model.coef_[0],\n",
    "    'abs_coefficient': np.abs(lr_model.coef_[0])\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print('Logistic Regression Feature Importance:')\n",
    "print(lr_feature_importance)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(lr_feature_importance['feature'], lr_feature_importance['coefficient'])\n",
    "plt.title('Logistic Regression - Feature Coefficients')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=20)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "dt_train_pred = dt_model.predict(X_train)\n",
    "dt_test_pred = dt_model.predict(X_test)\n",
    "dt_test_proba = dt_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "dt_train_accuracy = accuracy_score(y_train, dt_train_pred)\n",
    "dt_test_accuracy = accuracy_score(y_test, dt_test_pred)\n",
    "dt_auc = roc_auc_score(y_test, dt_test_proba)\n",
    "\n",
    "print('=== DECISION TREE RESULTS ===')\n",
    "print(f'Training Accuracy: {dt_train_accuracy:.4f}')\n",
    "print(f'Test Accuracy: {dt_test_accuracy:.4f}')\n",
    "print(f'AUC Score: {dt_auc:.4f}')\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, dt_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Decision Tree\n",
    "dt_feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': dt_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print('Decision Tree Feature Importance:')\n",
    "print(dt_feature_importance)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(dt_feature_importance['feature'], dt_feature_importance['importance'])\n",
    "plt.title('Decision Tree - Feature Importance')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree'],\n",
    "    'Training Accuracy': [lr_train_accuracy, dt_train_accuracy],\n",
    "    'Test Accuracy': [lr_test_accuracy, dt_test_accuracy],\n",
    "    'AUC Score': [lr_auc, dt_auc]\n",
    "})\n",
    "\n",
    "print('=== MODEL COMPARISON ===')\n",
    "print(comparison_df)\n",
    "\n",
    "# Determine best model\n",
    "best_model_idx = comparison_df['Test Accuracy'].idxmax()\n",
    "best_model = comparison_df.loc[best_model_idx, 'Model']\n",
    "print(f'\\nBest performing model: {best_model}')\n",
    "print(f'Test Accuracy: {comparison_df.loc[best_model_idx, \"Test Accuracy\"]:.4f}')\n",
    "print(f'AUC Score: {comparison_df.loc[best_model_idx, \"AUC Score\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, comparison_df['Training Accuracy'], width, label='Training', alpha=0.8)\n",
    "axes[0].bar(x + width/2, comparison_df['Test Accuracy'], width, label='Test', alpha=0.8)\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(comparison_df['Model'])\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# AUC comparison\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['AUC Score'], alpha=0.8)\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('AUC Score')\n",
    "axes[1].set_title('AUC Score Comparison')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ROC Curves\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_test_proba)\n",
    "dt_fpr, dt_tpr, _ = roc_curve(y_test, dt_test_proba)\n",
    "\n",
    "axes[2].plot(lr_fpr, lr_tpr, label=f'Logistic Regression (AUC = {lr_auc:.3f})', linewidth=2)\n",
    "axes[2].plot(dt_fpr, dt_tpr, label=f'Decision Tree (AUC = {dt_auc:.3f})', linewidth=2)\n",
    "axes[2].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "axes[2].set_title('ROC Curves')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Logistic Regression confusion matrix\n",
    "lr_cm = confusion_matrix(y_test, lr_test_pred)\n",
    "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Logistic Regression\\nConfusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Decision Tree confusion matrix\n",
    "dt_cm = confusion_matrix(y_test, dt_test_pred)\n",
    "sns.heatmap(dt_cm, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title('Decision Tree\\nConfusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation and Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "print('=== CROSS-VALIDATION RESULTS ===')\n",
    "\n",
    "# Logistic Regression CV\n",
    "lr_cv_scores = cross_val_score(lr_model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "print(f'Logistic Regression CV Scores: {lr_cv_scores}')\n",
    "print(f'Logistic Regression CV Mean: {lr_cv_scores.mean():.4f} (+/- {lr_cv_scores.std() * 2:.4f})')\n",
    "\n",
    "# Decision Tree CV\n",
    "dt_cv_scores = cross_val_score(dt_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f'Decision Tree CV Scores: {dt_cv_scores}')\n",
    "print(f'Decision Tree CV Mean: {dt_cv_scores.mean():.4f} (+/- {dt_cv_scores.std() * 2:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Decision Tree\n",
    "print('Performing hyperparameter tuning for Decision Tree...')\n",
    "\n",
    "dt_param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [10, 20, 50],\n",
    "    'min_samples_leaf': [5, 10, 20]\n",
    "}\n",
    "\n",
    "dt_grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    dt_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "dt_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {dt_grid_search.best_params_}')\n",
    "print(f'Best CV score: {dt_grid_search.best_score_:.4f}')\n",
    "\n",
    "# Train optimized model\n",
    "dt_optimized = dt_grid_search.best_estimator_\n",
    "dt_opt_test_pred = dt_optimized.predict(X_test)\n",
    "dt_opt_test_accuracy = accuracy_score(y_test, dt_opt_test_pred)\n",
    "\n",
    "print(f'Optimized Decision Tree Test Accuracy: {dt_opt_test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Making Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample new data for prediction\n",
    "new_customers = pd.DataFrame({\n",
    "    'TimeOnSite': [15.5, 8.2, 25.0, 5.1],\n",
    "    'Age': [35, 28, 45, 22],\n",
    "    'Gender_encoded': [1, 0, 1, 0],  # 1=Male, 0=Female (based on our encoding)\n",
    "    'AdsClicked': [5, 2, 8, 1],\n",
    "    'PreviousPurchases': [2, 0, 4, 1]\n",
    "})\n",
    "\n",
    "print('New customer data:')\n",
    "print(new_customers)\n",
    "\n",
    "# Scale the new data\n",
    "new_customers_scaled = scaler.transform(new_customers)\n",
    "\n",
    "# Make predictions with both models\n",
    "lr_new_pred = lr_model.predict(new_customers_scaled)\n",
    "lr_new_proba = lr_model.predict_proba(new_customers_scaled)[:, 1]\n",
    "\n",
    "dt_new_pred = dt_optimized.predict(new_customers)\n",
    "dt_new_proba = dt_optimized.predict_proba(new_customers)[:, 1]\n",
    "\n",
    "# Create prediction results\n",
    "prediction_results = new_customers.copy()\n",
    "prediction_results['LR_Prediction'] = lr_new_pred\n",
    "prediction_results['LR_Probability'] = lr_new_proba\n",
    "prediction_results['DT_Prediction'] = dt_new_pred\n",
    "prediction_results['DT_Probability'] = dt_new_proba\n",
    "\n",
    "print('\\nPrediction Results:')\n",
    "print(prediction_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== FINAL SUMMARY ===')\n",
    "print()\n",
    "print('Dataset Information:')\n",
    "print(f'- Total samples: {len(df_clean)}')\n",
    "print(f'- Features: {len(feature_columns)}')\n",
    "print(f'- Purchase rate: {df_clean[\"Purchase\"].mean():.2%}')\n",
    "print()\n",
    "print('Model Performance:')\n",
    "print(f'- Logistic Regression Test Accuracy: {lr_test_accuracy:.4f}')\n",
    "print(f'- Decision Tree Test Accuracy: {dt_test_accuracy:.4f}')\n",
    "print(f'- Optimized Decision Tree Test Accuracy: {dt_opt_test_accuracy:.4f}')\n",
    "print()\n",
    "print('Key Insights:')\n",
    "print('1. Most important features for purchase prediction:')\n",
    "for i, (feature, importance) in enumerate(dt_feature_importance.head(3).values, 1):\n",
    "    print(f'   {i}. {feature}: {importance:.4f}')\n",
    "print()\n",
    "print('2. Model Recommendations:')\n",
    "if lr_test_accuracy > dt_test_accuracy:\n",
    "    print('   - Logistic Regression performs better for this dataset')\n",
    "    print('   - Provides good interpretability with feature coefficients')\n",
    "else:\n",
    "    print('   - Decision Tree performs better for this dataset')\n",
    "    print('   - Provides clear feature importance and decision rules')\n",
    "print()\n",
    "print('3. Business Applications:')\n",
    "print('   - Use model to identify high-value customers')\n",
    "print('   - Optimize marketing campaigns based on feature importance')\n",
    "print('   - Personalize website experience to increase conversion')\n",
    "print('   - Focus on customers with high purchase probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and preprocessing objects\n",
    "import joblib\n",
    "\n",
    "# Determine best model\n",
    "if lr_test_accuracy > dt_opt_test_accuracy:\n",
    "    best_model_final = lr_model\n",
    "    best_model_name = 'logistic_regression'\n",
    "    use_scaling = True\n",
    "else:\n",
    "    best_model_final = dt_optimized\n",
    "    best_model_name = 'decision_tree'\n",
    "    use_scaling = False\n",
    "\n",
    "# Save model and scaler\n",
    "joblib.dump(best_model_final, f'{best_model_name}_model.pkl')\n",
    "if use_scaling:\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(le_gender, 'gender_encoder.pkl')\n",
    "\n",
    "print(f'Best model ({best_model_name}) saved successfully!')\n",
    "print('Files saved:')\n",
    "print(f'- {best_model_name}_model.pkl')\n",
    "if use_scaling:\n",
    "    print('- scaler.pkl')\n",
    "print('- gender_encoder.pkl')\n",
    "\n",
    "# Create a simple prediction function\n",
    "def predict_purchase(time_on_site, age, gender, ads_clicked, previous_purchases):\n",
    "    \"\"\"\n",
    "    Predict purchase likelihood for a new customer\n",
    "    \"\"\"\n",
    "    # Encode gender\n",
    "    gender_encoded = le_gender.transform([gender])[0]\n",
    "    \n",
    "    # Create feature array\n",
    "    features = np.array([[time_on_site, age, gender_encoded, ads_clicked, previous_purchases]])\n",
    "    \n",
    "    # Scale if needed\n",
    "    if use_scaling:\n",
    "        features = scaler.transform(features)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = best_model_final.predict(features)[0]\n",
    "    probability = best_model_final.predict_proba(features)[0, 1]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Test the prediction function\n",
    "test_prediction, test_probability = predict_purchase(15.5, 35, 'Male', 5, 2)\n",
    "print(f'\\nTest prediction: {test_prediction} (probability: {test_probability:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Project Complete! ðŸŽ‰\n",
    "\n",
    "This notebook successfully implemented a complete machine learning pipeline for predicting product purchase likelihood. The project included:\n",
    "\n",
    "âœ… **Data Loading & Exploration**\n",
    "âœ… **Data Cleaning & Preprocessing**\n",
    "âœ… **Exploratory Data Analysis**\n",
    "âœ… **Feature Engineering**\n",
    "âœ… **Model Training** (Logistic Regression & Decision Tree)\n",
    "âœ… **Model Evaluation & Comparison**\n",
    "âœ… **Hyperparameter Tuning**\n",
    "âœ… **Cross-Validation**\n",
    "âœ… **Prediction on New Data**\n",
    "âœ… **Model Deployment Preparation**\n",
    "\n",
    "The models can now be used to predict customer purchase behavior in e-commerce applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}